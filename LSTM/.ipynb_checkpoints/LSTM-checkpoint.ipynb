{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67920ca-eca2-46be-8a10-7c5ae0407cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3 as sq\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, optimizers, callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, SimpleRNN, Activation, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bbb0b-bdae-4652-8a63-e6a182fc4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sq.connect('C:/Users/Reza Fakhrurrozi/Documents/GitHub/PlatinumChallange-Group2-/database_pl.db', check_same_thread = False)\n",
    "q_data = 'SELECT * FROM data'\n",
    "df = pd.read_sql_query(q_data, db)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57d19c-bdc5-4752-b935-b424081e7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01344f-6389-4035-b174-ed794ec3e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memfilter data berdasarkan label\n",
    "df_positive = df[df['Label'] == 'positive']\n",
    "df_negative = df[df['Label'] == 'negative']\n",
    "df_neutral = df[df['Label'] == 'neutral']\n",
    "\n",
    "# menyeimbangkan label netral dengan label negatif\n",
    "df_neutral_over = df_neutral.sample(df_negative.shape[0], replace=True)\n",
    "\n",
    "#menggabungan semua data\n",
    "df = pd.concat([df_positive, df_negative, df_neutral_over])\n",
    "\n",
    "# mengecek kembali\n",
    "label_counts = df['Label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1146ddb-b489-4c03-9f9e-bd7d288ca193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data tweet\n",
    "positive_tweet = df.loc[df['Label']=='positive'].Tweet.tolist()\n",
    "negative_tweet = df.loc[df['Label']=='negative'].Tweet.tolist()\n",
    "neutral_tweet = df.loc[df['Label']=='neutral'].Tweet.tolist()\n",
    "\n",
    "# Group df label\n",
    "positive_label = df.loc[df['Label']=='positive'].Label.tolist()\n",
    "negative_label = df.loc[df['Label']=='negative'].Label.tolist()\n",
    "neutral_label = df.loc[df['Label']=='neutral'].Label.tolist()\n",
    "\n",
    "total_data = positive_tweet + negative_tweet + neutral_tweet\n",
    "labels = positive_label + negative_label + neutral_label\n",
    "\n",
    "print(\"Positive: %s, Negative: %s, Neutral: %s\" % (len(positive_tweet), len(neutral_tweet), len(negative_tweet)))\n",
    "print(\"Total data: %s\" % len(total_data))\n",
    "print(\"Total labels: %s\" % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aeba09-ef1b-4731-a04e-1f19bda4bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', lower=True)\n",
    "tokenizer.fit_on_texts(total_data)\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"tokenizer.pickle has created!\")\n",
    "\n",
    "X = tokenizer.texts_to_sequences(total_data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "maxlen = max(len(x) for x in X)\n",
    "\n",
    "X = pad_sequences(X)\n",
    "with open('x_pad_sequences.pickle', 'wb') as handle:\n",
    "    pickle.dump(X, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"x_pad_sequences.pickle has created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b20f86-7978-466c-b0c4-655f64ddc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(labels)\n",
    "Y = Y.values\n",
    "\n",
    "with open('y_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(Y, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"y_labels.pickle has created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9492b-1039-4260-b130-54c8a764df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"x_pad_sequences.pickle\",'rb')\n",
    "X = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"y_labels.pickle\",'rb')\n",
    "Y = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47967326-35b0-48de-b753-145cad8ddb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "y = Y\n",
    "\n",
    "embed_dim = 200\n",
    "units = 64\n",
    "\n",
    "for iteration, data in enumerate(kf.split(X), start=1):\n",
    "\n",
    "    data_train   = X[data[0]]\n",
    "    target_train = y[data[0]]\n",
    "\n",
    "    data_test    = X[data[1]]\n",
    "    target_test  = y[data[1]]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
    "    model.add(LSTM(units, dropout=0.8))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate = 0.00005)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[es])\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    y_pred = predictions\n",
    "\n",
    "    # for the current fold only\n",
    "    \n",
    "    accuracy = accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "    print(\"Training ke-\", iteration)\n",
    "    print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"Rata-rata Accuracy: \", round(average_accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c886f0-d741-4631-bd8f-41e0b1903d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b-', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r-', label='Validation acc')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b-', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_history(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa7e6b-6dc4-4f88-af32-158cf5c424da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "model.save('model.h5')\n",
    "print(\"Model has created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8724c3f-01fc-43a5-8aee-8c1887093b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Rasa bersyukur, cukup.\"\"\"\n",
    "\n",
    "def cleansing(sent):\n",
    "    string = sent.lower()\n",
    "    string = re.sub(r'[^a-zA-Z0-9]', ' ', string)\n",
    "    return string\n",
    "\n",
    "sentiment = ['negative', 'neutral', 'positive']\n",
    "\n",
    "text = [cleansing(input_text)]\n",
    "predicted = tokenizer.texts_to_sequences(text)\n",
    "guess = pad_sequences(predicted, maxlen=X.shape[1])\n",
    "\n",
    "model = load_model('model.h5')\n",
    "prediction = model.predict(guess)\n",
    "polarity = np.argmax(prediction[0])\n",
    "hasil = sentiment[polarity]\n",
    "\n",
    "print(\"Text: %s\" % text[0])\n",
    "print(\"Sentiment: %s\" % sentiment[polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721fbb1-f171-4c2a-8df2-e27434fe75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Makanannya busuk\"\"\"\n",
    "\n",
    "def cleansing(sent):\n",
    "    string = sent.lower()\n",
    "    string = re.sub(r'[^a-zA-Z0-9]', ' ', string)\n",
    "    return string\n",
    "\n",
    "sentiment = ['negative', 'neutral', 'positive']\n",
    "\n",
    "text = [cleansing(input_text)]\n",
    "predicted = tokenizer.texts_to_sequences(text)\n",
    "guess = pad_sequences(predicted, maxlen=X.shape[1])\n",
    "\n",
    "model = load_model('model.h5')\n",
    "prediction = model.predict(guess)\n",
    "polarity = np.argmax(prediction[0])\n",
    "hasil = sentiment[polarity]\n",
    "\n",
    "print(\"Text: %s\" % text[0])\n",
    "print(\"Sentiment: %s\" % sentiment[polarity])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
